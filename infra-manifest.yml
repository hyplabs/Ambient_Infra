# TODO: ADD SSL CERTS
# TODO: ADD PUBLIC FACING GATEWAY

# GCGO SERVER

apiVersion: apps/v1
kind: Deployment
metadata:
  name: graphcache-go-deployment
  labels:
    app: graphcache-go
spec:
  replicas: 1
  selector:
    matchLabels:
      app: graphcache-go
  template:
    metadata:
      labels:
        app: graphcache-go
    spec:
      containers:
        - name: graphcache-go
          image: cadehypotenuse/graphcache-go:latest
          ports:
            - containerPort: 8080 # Adjust the port if your application listens on a different one

#load balancer settings
---
apiVersion: v1
kind: Service
metadata:
  name: graphcache-go-service
spec:
  selector:
    app: graphcache-go
  ports:
    - protocol: TCP
      port: 80 # The port the service listens on
      targetPort: 8080 # The port the application inside the container listens on
  type: LoadBalancer # Exposes the Service externally using a cloud provider's load balancer

# ANALYTICS TOOLS

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: analytics-tools
  labels:
    app: analytics-tools
spec:
  replicas: 1
  selector:
    matchLabels:
      app: analytics-tools
  template:
    metadata:
      labels:
        app: analytics-tools
    spec:
      containers:
        - name: crocswap-analytics-tools-container
          image: cadehypotenuse/crocswap_audit_tools:latest # Make sure this is the correct image name and tag
          command: ["python3", "run_server.py"] # Will start the actual server inside
          ports:
            - containerPort: 8080 # Adjust the port as needed
          # TODO: Understand ramifications of setting resource limits
          # Commented out as it lead to a unschedulable pod
          # resources:
          #   requests:
          #     cpu: "4000m"
          #     memory: "2Gi"
          #   limits:
          #     memory: "2Gi"
          envFrom:
            - secretRef:
                name: secrets-analytic-tools

#load balancer settings
---
apiVersion: v1
kind: Service
metadata:
  name: analytics-tools-service
spec:
  selector:
    app: analytics-tools
  ports:
    - protocol: http
      port: 80 # The port the service listens on
      targetPort: 8080 # The port the application inside the container listens on
  type: LoadBalancer # Exposes the Service externally using a cloud provider's load balancer

# GCGO CANDLES
apiVersion: apps/v1
kind: Deployment
metadata:
  name: graphcache-go-candles-deployment
  labels:
    app: graphcache-go-candles
spec:
  replicas: 1
  selector:
    matchLabels:
      app: graphcache-go-candles
  template:
    metadata:
      labels:
        app: graphcache-go-candles
    spec:
      containers:
        - name: graphcache-go-candles
          image: cadehypotenuse/graphcache-go-candles:latest
          ports:
            - containerPort: 8080 # Adjust the port if your application listens on a different one
          envFrom:
            - secretRef:    # Reference the ConfigMap here
                name: env-gcgo-candles

#load balancer settings
---
apiVersion: v1
kind: Service
metadata:
  name: graphcache-go-candles-service
spec:
  selector:
    app: graphcache-go-candles
  ports:
    - protocol: TCP
      port: 80 # The port the service listens on
      targetPort: 8080 # The port the application inside the container listens on
  type: LoadBalancer # Exposes the Service externally using a cloud provider's load balancer

# CHAT SERVER
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: chat-server-deployment
  labels:
    app: graphcache
spec:
  # The websocket library of the chat server does requires session afinity (which the
  # current ingress doesn't support). Therefore we can only one 1 replica at a time,
  # any more will break client. For current chat workloads this should be sufficient.
  replicas: 1
  selector:
    matchLabels:
      tier: chat-servers
  template:
    metadata:
      labels:
        tier: chat-servers
    spec:
      containers:
      - name: chat-server
        image: anasonmania/express-test:latest
        ports:
        - containerPort: 5000
        # resources:
        #   requests:
        #     cpu: "4000m"
        #     memory: "2Gi"
        #   limits:
        #     memory: "2Gi"
        # env:
        # - name: PROTOCOL
        #   value: "http"
        # - name: PORT
        #   value: "5000"
        # - name: MONGO_USER
        #   value: "chat-server"
        # - name: MONGO_SERVER
        #   value: "serverlessinstance0.swuv52l.mongodb.net"
        # - name: MONGO_PWD
        #   valueFrom:
        #     secretKeyRef:
        #       name: mongo-creds
        #       key: mongo-pwd
        livenessProbe:
          httpGet:
            path: "/chat/api/status"
            port: 5000
          initialDelaySeconds: 3
          periodSeconds: 3
          failureThreshold: 3
        envFrom:
          - secretRef:
              name: secrets-analytic-tools

#load balancer settings
---
apiVersion: v1
kind: Service
metadata:
  name: chat-service
  labels:
    app: graphcache
  annotations:
    networking.gke.io/load-balancer-type: "Internal"
    cloud.google.com/backend-config: '{"default": "stick-service-backend-cfg"}'
spec:
  type: LoadBalancer
  selector:
    tier: chat-servers
  ports:
  - name: http
    port: 80
    targetPort: 5000